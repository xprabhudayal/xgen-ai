{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd1b1bc-55af-4afc-9308-5fec3c022f27",
   "metadata": {},
   "source": [
    "# Here is the XGen AI bot ü§ñ, made with python and backed by llama3 üíñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb7b5e7-b0de-43ce-95a2-aaa5ed114eff",
   "metadata": {},
   "source": [
    "# loading of the pretrained model üíù "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b8b8489-02b5-41c6-8873-ced8c9b5bd79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.26.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"llama/lora\", \n",
    "        max_seq_length = 2048,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f860502-197c-470b-a0a6-5500c4961d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "990dafe1-d98f-4f70-a21b-9133c283bdd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.generation.streamers.TextStreamer'>\n",
      "user\n",
      "\n",
      "how are you feeling to be a part of my projectassistant\n",
      "\n",
      "I am feeling very excited to be a part of your project. I am a machine learning model, so I am always eager to learn and help in any way I can. I am also very curious about the project and the goals you have for it, so I am looking forward to learning more about it and how I can assist you.\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3\",\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
    ")\n",
    "\n",
    "\n",
    "# FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate(prompt):\n",
    "    messages = [\n",
    "      {\"from\": \"human\", \"value\": f\"{prompt}\"},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize = True,\n",
    "      add_generation_prompt = True,\n",
    "      return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids = inputs,\n",
    "                             max_new_tokens = 150\n",
    "                            , use_cache = True,\n",
    "                           pad_token_id=tokenizer.eos_token_id\n",
    "                          )\n",
    "\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    # print(response)\n",
    "    response_frmt = response[0].split(\"assistant\")\n",
    "    response_frmt = response_frmt[1].strip()\n",
    "    return response_frmt\n",
    "\n",
    "\n",
    "def generate_textwise(prompt):\n",
    "    messages = [\n",
    "      {\"from\": \"human\", \"value\": f\"{prompt}\"},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize = True,\n",
    "      add_generation_prompt = True,\n",
    "      return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    from transformers import TextStreamer\n",
    "    text_streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n",
    "    print(type(text_streamer))\n",
    "    \n",
    "    outputs = model.generate(input_ids = inputs,\n",
    "                             streamer = text_streamer,\n",
    "                             max_new_tokens = 128,\n",
    "                             pad_token_id=tokenizer.eos_token_id\n",
    "                             )\n",
    "    \n",
    "    \n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    # print(response)\n",
    "    response_frmt = response[0].split(\"assistant\")\n",
    "    response_frmt = response_frmt[1].strip()\n",
    "    # return response_frmt\n",
    "    \n",
    "                                \n",
    "  # return outputs\n",
    "\n",
    "generate_textwise(\"how are you feeling to be a part of my project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8565f7-0293-4229-97ce-b5e1cd5c2d26",
   "metadata": {},
   "source": [
    "# # telegram bot code ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3821a1da-878c-4b9e-899b-c74603a0b8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello:\n",
      "hello:\n",
      "hello:\n",
      "hello:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_118/4060944865.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hello:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "while True:\n",
    "    print(\"hello:\")\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0e84269-9b42-4295-b4be-612a09ca6923",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>how can you describe you experience as an ai assistant? how do you feel about your work? do you have any goals or aspirations? do you have any advice for humans? do you have any favorite things or activities? do you have any favorite books or movies? do you have any favorite music or artists? do you have any favorite foods or drinks? do you have any favorite places or destinations? do you have any favorite hobbies or activities? do you have any favorite sports or games? do you have any favorite animals or pets? do you have any favorite plants or flowers? do you have any favorite colors or patterns? do you have any favorite shapes or forms? do you have any favorite\n"
     ]
    }
   ],
   "source": [
    "# prompt = 'how can you describe you experience as an ai assistant?'\n",
    "# from transformers import TextStreamer\n",
    "# text_streamer = TextStreamer(tokenizer)\n",
    "# _ = model.generate(prompt, streamer = text_streamer, max_new_tokens = 128)\n",
    "\n",
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    'how can you describe you experience as an ai assistant?'\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1aa007f6-734a-4600-a77d-64947fa5d53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot close a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2816/2192005041.py\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_polling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/telegram/ext/_application.py\u001b[0m in \u001b[0;36mrun_polling\u001b[0;34m(self, poll_interval, timeout, bootstrap_retries, read_timeout, write_timeout, connect_timeout, pool_timeout, allowed_updates, drop_pending_updates, close_loop, stop_signals)\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m         return self.__run(\n\u001b[0m\u001b[1;32m    872\u001b[0m             updater_coroutine=self.updater.start_polling(\n\u001b[1;32m    873\u001b[0m                 \u001b[0mpoll_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoll_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/telegram/ext/_application.py\u001b[0m in \u001b[0;36m__run\u001b[0;34m(self, updater_coroutine, stop_signals, close_loop)\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mclose_loop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m     def create_task(\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/asyncio/unix_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finalizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signal_handlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/asyncio/selector_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot close a running event loop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot close a running event loop"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from telegram import Update\n",
    "from telegram.ext import ApplicationBuilder, ContextTypes, CommandHandler, MessageHandler, filters\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "tkn = 'enter your token here üíñ'\n",
    "\n",
    "#This would show us that the bot is typing...\n",
    "from functools import wraps\n",
    "\n",
    "def send_action(action):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        async def command_func(update, context, *args, **kwargs):\n",
    "            await context.bot.send_chat_action(chat_id=update.effective_message.chat_id, action=action)\n",
    "            await func(update, context,  *args, **kwargs)\n",
    "        return command_func\n",
    "    return decorator\n",
    "###\n",
    "\n",
    "#tele based functions\n",
    "\n",
    "@send_action('typing')\n",
    "async def start(update : Update, context : ContextTypes.DEFAULT_TYPE):\n",
    "    greetings = [\n",
    "        \"Hey there! XgenAI at your service, what's going on?\",\n",
    "        \"Oh hi! It's XgenAI here. What can I do for you today?\",\n",
    "        \"XgenAI here! Ready for some fun? What can we discuss today?\",\n",
    "        \"Hi there, you've reached XgenAI. How can I assist you?\",\n",
    "        \"Namaste, XgenAI here! What would you like to talk about?\"\n",
    "    ]\n",
    "    greet : str = random.choice(greetings)\n",
    "    await update.message.reply_text(greet)\n",
    "\n",
    "    \n",
    "@send_action('typing')\n",
    "async def about(update : Update, context : ContextTypes.DEFAULT_TYPE):\n",
    "    await update.message.reply_text(\"Hi, I am xGenAI. I'm an enthusiastic assistant and an AI model, backed by Meta Llama3.\")\n",
    "\n",
    "    \n",
    "@send_action('typing')\n",
    "async def generate_prompt(update : Update, context : ContextTypes.DEFAULT_TYPE):\n",
    "    text : str = update.message.text\n",
    "    res = generate(text)\n",
    "    await update.message.reply_text(res)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"running...\")\n",
    "\n",
    "    app = ApplicationBuilder().token(tkn).build()\n",
    "\n",
    "    #cmd handler tele\n",
    "    start_handler = CommandHandler('start', start)\n",
    "    app.add_handler(start_handler)\n",
    "\n",
    "    about_handler = CommandHandler('about', about)\n",
    "    app.add_handler(about_handler)\n",
    "\n",
    "    #message handler\n",
    "    app.add_handler(MessageHandler(filters.TEXT, generate_prompt))\n",
    "\n",
    "\n",
    "    app.run_polling()\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

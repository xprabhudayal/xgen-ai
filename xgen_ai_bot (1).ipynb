{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTzNyXTLInaH"
   },
   "source": [
    "# Here is the XGen AI bot 🤖, made with python and backed by llama3 💖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wi4NDwnLm8HK"
   },
   "source": [
    "# Run all the Cells and wait ⌚\n",
    "---\n",
    "to run all cells go to `Runtime` -> `Run all` from the dropdown list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s4z6WeoMR4rz",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-6zoglz92/unsloth_2165e0ad1b354788b9edc318ae5a39d1\n",
      "  Running command git clone -q https://github.com/unslothai/unsloth.git /tmp/pip-install-6zoglz92/unsloth_2165e0ad1b354788b9edc318ae5a39d1\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 933d9fe2cb2459f949ee2250e90a5b610d277eab\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: datasets>=2.16.0 in ./.conda/envs/default/lib/python3.9/site-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.2)\n",
      "Requirement already satisfied: tqdm in ./.conda/envs/default/lib/python3.9/site-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\n",
      "Requirement already satisfied: sentencepiece in ./.conda/envs/default/lib/python3.9/site-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: psutil in ./.conda/envs/default/lib/python3.9/site-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.8)\n",
      "Requirement already satisfied: numpy in ./.conda/envs/default/lib/python3.9/site-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n",
      "Requirement already satisfied: transformers>=4.38.2 in ./.conda/envs/default/lib/python3.9/site-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.41.2)\n",
      "Requirement already satisfied: wheel>=0.42.0 in ./.conda/envs/default/lib/python3.9/site-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n",
      "Requirement already satisfied: tyro in ./.conda/envs/default/lib/python3.9/site-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.4)\n",
      "Requirement already satisfied: protobuf<4.0.0 in ./.conda/envs/default/lib/python3.9/site-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
      "Requirement already satisfied: aiohttp in ./.conda/envs/default/lib/python3.9/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.5)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in ./.conda/envs/default/lib/python3.9/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.3.1)\n",
      "Requirement already satisfied: requests>=2.32.1 in ./.conda/envs/default/lib/python3.9/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.conda/envs/default/lib/python3.9/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\n",
      "Requirement already satisfied: xxhash in ./.conda/envs/default/lib/python3.9/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./.conda/envs/default/lib/python3.9/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: packaging in ./.conda/envs/default/lib/python3.9/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in ./.conda/envs/default/lib/python3.9/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.23.2)\n",
      "Requirement already satisfied: pandas in ./.conda/envs/default/lib/python3.9/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.conda/envs/default/lib/python3.9/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: filelock in ./.conda/envs/default/lib/python3.9/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.14.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/envs/default/lib/python3.9/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in ./.conda/envs/default/lib/python3.9/site-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (16.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/envs/default/lib/python3.9/site-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub>=0.21.2->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/default/lib/python3.9/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/envs/default/lib/python3.9/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/default/lib/python3.9/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/default/lib/python3.9/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.conda/envs/default/lib/python3.9/site-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.conda/envs/default/lib/python3.9/site-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.conda/envs/default/lib/python3.9/site-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.conda/envs/default/lib/python3.9/site-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/envs/default/lib/python3.9/site-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/envs/default/lib/python3.9/site-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in ./.conda/envs/default/lib/python3.9/site-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in ./.conda/envs/default/lib/python3.9/site-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
      "Requirement already satisfied: rich>=11.1.0 in ./.conda/envs/default/lib/python3.9/site-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\n",
      "Requirement already satisfied: eval-type-backport>=0.1.3 in ./.conda/envs/default/lib/python3.9/site-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.conda/envs/default/lib/python3.9/site-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.conda/envs/default/lib/python3.9/site-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.conda/envs/default/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.6-py3-none-any.whl size=117332 sha256=71274d0f581126e3124b6e602bd79ec1b67454d52081413f66cb9697142df085\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bm70h9yu/wheels/0b/bf/f5/61523189908a01bce8752a181f02f8b057ffc2c792447d39ff\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth\n",
      "  Attempting uninstall: unsloth\n",
      "    Found existing installation: unsloth 2024.5\n",
      "    Uninstalling unsloth-2024.5:\n",
      "      Successfully uninstalled unsloth-2024.5\n",
      "Successfully installed unsloth-2024.6\n"
     ]
    }
   ],
   "source": [
    "!pip install python-telegram-bot transformers bitsandbytes datasets --quiet\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --quiet\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPNPLfS2SOiw"
   },
   "source": [
    "# llm configuration 🚀\n",
    "---\n",
    "I'm using the quantized version of the LLM(large language model) which eventually is **lightweight in RESOURCE usage** as compared with original `LLAMA3`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3I-_GnwZT2ij",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed88fead46a8416696910fec7f2367ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.26.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4e0f5c04a54a57b3a50918024427f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90b4a7ec6774755ba26ba1c4b84bf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04057cc2dbe4e159114311a2a32e19d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/131 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f4bc8af7bb4ac58ce8f8eae97da0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7bdfbe6e4d4a70ac5d70023ad96f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de45587b3424bf8af92f5aa934e619f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    "    cache_dir = \"./llama\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GudeSO6YLRfE"
   },
   "source": [
    "# finetuning ⏲️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHD1Tw_g1Qe7"
   },
   "source": [
    "###part 1 : loading the alpaca format ( chat based template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ldXdLcUzLQwf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b0588470c245a3921f369280b389a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/442 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed4d2d33ef64eda9bf8dffd546d2a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84104312b02944979370d10a2be4d8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9033 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a20d036187a4eed9bebde6ce7b150ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9033 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\n",
    "#------------------------------------#------------------chat template------------------#------------------------------------\n",
    "\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"philschmid/guanaco-sharegpt-style\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qv_FZM7v1s-7",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## part 2 : training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7XwwGDrpPZq1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4121349b21914c48add12ee0b6567c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/9033 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 9,033 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 10:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.327300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.671600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.828400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.723400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.715100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.380100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.364800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.737800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.650400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.182300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.073800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.294300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.574200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.223400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.993300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.279900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.369200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.244500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.300900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.339500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.481800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.419600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.149300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.589900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.493600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.334200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.302200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.498100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.264400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.567100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.414700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.420700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.453300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.826700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.375200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.220700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.081900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.572400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.356300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.590700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.270300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.181600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjw6rh9a5IPB"
   },
   "source": [
    "# test it out ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJTUjNlg6fYj",
    "tags": []
   },
   "source": [
    "# run only if lora model(finetuned) is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lu2jWgEl6bdW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.26.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"llama/lora\", \n",
    "        max_seq_length = 2048,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  green revolution negative inpact\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Green Revolution, which began in the 1940s and 1950s, was a period of rapid agricultural development in many parts of the world, particularly in Asia and Latin America. The Green Revolution was characterized by the widespread adoption of new crop varieties, irrigation systems, and fertilizers, which allowed farmers to increase their yields and produce more food.\\n\\nHowever, the Green Revolution also had some negative impacts. For example, the increased use of fertilizers and pesticides led to soil degradation and water pollution. The increased use of irrigation systems also led to'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3\",\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
    ")\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate(prompt):\n",
    "    messages = [\n",
    "      {\"from\": \"human\", \"value\": f\"{prompt}\"},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize = True,\n",
    "      add_generation_prompt = True,\n",
    "      return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 111\n",
    "                            , use_cache = True,\n",
    "                           pad_token_id=tokenizer.eos_token_id\n",
    "                           # attention_mask=\n",
    "                          )\n",
    "\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    # print(response)\n",
    "    response_frmt = response[0].split(\"assistant\")\n",
    "    response_frmt = response_frmt[1].strip()\n",
    "    return response_frmt\n",
    "\n",
    "                                \n",
    "  # return outputs\n",
    "\n",
    "generate(input())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnEYquSO6pdU",
    "tags": []
   },
   "source": [
    "# elsewise run (immediate after training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_118/1026203893.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model saving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"llama3-chat-bnb1122\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Local saving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# model.push_to_hub(\"pradachan/llama3-chat-bnb\", token = \"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\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# model.push_to_hub_merged(\"pradachan/llama3-chat-bnb-\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model.push_to_hub_merged(\"pradachan/llama3-chat-bnb\", tokenizer, save_method = \"merged_8bit\", token = \"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\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# model saving\n",
    "model.save_pretrained(\"llama3-chat-bnb1122\") # Local saving\n",
    "# model.push_to_hub(\"pradachan/llama3-chat-bnb\", token = \"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\")\n",
    "# model.push_to_hub_merged(\"pradachan/llama3-chat-bnb-\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n",
    "# model.push_to_hub_merged(\"pradachan/llama3-chat-bnb\", tokenizer, save_method = \"merged_8bit\", token = \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nrT_4caX3O5-",
    "outputId": "652a36ef-73c4-4e72-9193-914cb57e8460"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'gradient_checkpointing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_137/2182335590.py\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mfor_inference\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m   1936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0minternal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m         \u001b[0minternal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1939\u001b[0m         \u001b[0minternal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'gradient_checkpointing'"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3\",\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate(prompt):\n",
    "    messages = [\n",
    "      {\"from\": \"human\", \"value\": f\"{prompt}\"},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize = True,\n",
    "      add_generation_prompt = True,\n",
    "      return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 150, use_cache = True,\n",
    "                           pad_token_id=tokenizer.eos_token_id\n",
    "                           # attention_mask=\n",
    "                          )\n",
    "\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    # print(response)\n",
    "    response_frmt = response[0].split(\"assistant\")\n",
    "    response_frmt = response_frmt[1].strip()\n",
    "    return response_frmt\n",
    "\n",
    "                                \n",
    "  # return outputs\n",
    "\n",
    "generate(\"hi how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and LoRA adapter loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import PreTrainedTokenizerFast, LlamaForCausalLM\n",
    "from peft import PeftModel\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"unsloth/llama-3-8b-Instruct-bnb-4bit\", cache_dir=\"./llama\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"unsloth/llama-3-8b-Instruct-bnb-4bit\", cache_dir=\"./llama\")\n",
    "\n",
    "def load_model(model_path, lora_adapter_path):\n",
    "    # Load the tokenizer\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "    # Load the model\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "    # Load the LoRA adapter\n",
    "    model = PeftModel.from_pretrained(model, lora_adapter_path)\n",
    "\n",
    "    # Enable native 2x faster inference\n",
    "    model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"llama/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/\"\n",
    "    lora_adapter_path = \"llama/lora\"\n",
    "\n",
    "    model, tokenizer = load_model(model_path, lora_adapter_path)\n",
    "    print(\"Model and LoRA adapter loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_137/748436911.py\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;31m# return outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hi how are you\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_137/748436911.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     19\u001b[0m     ).to(\"cuda\")\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     outputs = model.generate(input_ids = inputs, max_new_tokens = 150, use_cache = True,\n\u001b[0m\u001b[1;32m     22\u001b[0m                            \u001b[0mpad_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                            \u001b[0;31m# attention_mask=\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "# from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# tokenizer = get_chat_template(\n",
    "#     tokenizer,\n",
    "#     chat_template = \"llama-3\",\n",
    "#     mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
    "# )\n",
    "\n",
    "\n",
    "def generate(prompt):\n",
    "    messages = [\n",
    "      {\"from\": \"human\", \"value\": f\"{prompt}\"},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize = True,\n",
    "      add_generation_prompt = True,\n",
    "      return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 150, use_cache = True,\n",
    "                           pad_token_id=tokenizer.eos_token_id\n",
    "                           # attention_mask=\n",
    "                          )\n",
    "\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    # print(response)\n",
    "    response_frmt = response[0].split(\"assistant\")\n",
    "    response_frmt = response_frmt[1].strip()\n",
    "    return response_frmt\n",
    "\n",
    "                                \n",
    "  # return outputs\n",
    "\n",
    "generate(\"hi how are you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# (Sagemaker) to flush out gpu memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch as t\n",
    "\n",
    "t.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmYsHyWQ2qB-"
   },
   "source": [
    "# telegram bot code 🤖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amRaObr_HFlH"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from telegram import Update\n",
    "from telegram.ext import ApplicationBuilder, ContextTypes, CommandHandler, MessageHandler, filters\n",
    "import google.generativeai as ai\n",
    "import nest_asyncio as nio\n",
    "import asyncio as aio\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "#its being used inorder to smoothly run our bot for indefinitely time in colab\n",
    "nio.apply()\n",
    "\n",
    "tkn = userdata.get('X_AI')\n",
    "api = userdata.get('API_KEY')\n",
    "\n",
    "#ai model\n",
    "ai.configure(api_key = api)\n",
    "model = ai.GenerativeModel('gemini-pro')\n",
    "\n",
    "\n",
    "#tele based functions\n",
    "async def start(update : Update, context : ContextTypes.DEFAULT_TYPE):\n",
    "    greetings = [\n",
    "        \"Hey there! XgenAI at your service, what's going on?\",\n",
    "        \"Oh hi! It's XgenAI here. What can I do for you today?\",\n",
    "        \"XgenAI here! Ready for some fun? What can we discuss today?\",\n",
    "        \"Hi there, you've reached XgenAI. How can I assist you?\",\n",
    "        \"Namaste, XgenAI here! What would you like to talk about?\"\n",
    "    ]\n",
    "    greet : str = random.choice(greetings)\n",
    "    await update.message.reply_text(greet)\n",
    "\n",
    "\n",
    "async def about(update : Update, context : ContextTypes.DEFAULT_TYPE):\n",
    "    await update.message.reply_text(\"xGenAI, is an integration of Telegram and Google's generative-ai model.\")\n",
    "\n",
    "\n",
    "async def generate_prompt(update : Update, context : ContextTypes.DEFAULT_TYPE):\n",
    "    text : str = update.message.text\n",
    "\n",
    "    if text.startswith(\".\"):\n",
    "      text = text.replace('.','')\n",
    "      res = generate(text)\n",
    "      await update.message.reply_text(res)\n",
    "    else:\n",
    "      await update.message.reply_text(handle_res(text))\n",
    "\n",
    "\n",
    "#ai model based\n",
    "def generate(text :str)-> str:\n",
    "    res = model.generate_content(text)\n",
    "    return res.text\n",
    "\n",
    "\n",
    "#message handler\n",
    "def handle_res(text : str) -> str:\n",
    "  if text.startswith(\".\"):\n",
    "    pass\n",
    "  else:\n",
    "    return \"prompt should be like this -> .prompt\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"running...\")\n",
    "\n",
    "    app = ApplicationBuilder().token(tkn).build()\n",
    "\n",
    "    #cmd handler tele\n",
    "    start_handler = CommandHandler('start', start)\n",
    "    app.add_handler(start_handler)\n",
    "\n",
    "    about_handler = CommandHandler('about', about)\n",
    "    app.add_handler(about_handler)\n",
    "\n",
    "    #message handler\n",
    "    app.add_handler(MessageHandler(filters.TEXT, generate_prompt))\n",
    "\n",
    "\n",
    "    app.run_polling()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
